{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import dataloader as dataloader\n",
    "import nn_model as nn_model\n",
    "import imputation as imp\n",
    "from baseline_utils import BaselineUtilTensor\n",
    "\n",
    "from captum.attr import IntegratedGradients, DeepLift\n",
    "from collections import defaultdict\n",
    "import shap\n",
    "\n",
    "rs = 42\n",
    "np.random.seed(rs)\n",
    "torch.manual_seed(rs)\n",
    "random.seed(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "learning_rate = 4e-3\n",
    "N = 100\n",
    "shap_sample_size = 10\n",
    "#possible blur/mean/zero\n",
    "imputation_typ = 'blur'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_gradients(X, baseline, model):\n",
    "    start_time = time.time()\n",
    "    ig = IntegratedGradients(model)\n",
    "    X = X.cuda()\n",
    "    baseline = baseline.cuda()\n",
    "    attr = ig.attribute(X, baseline, target=0)\n",
    "    print(\"--- '%.2f' seconds computation time ---\" % (time.time() - start_time))\n",
    "    return attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_gradients_mdb(X, model, X_min, X_max, columns):\n",
    "    start_time = time.time()\n",
    "    ig = IntegratedGradients(model)\n",
    "    attr = []\n",
    "    but = BaselineUtilTensor()\n",
    "    for observation in X:\n",
    "        cur_baseline = but.create_max_dist_baseline(observation, X_min, X_max, columns).unsqueeze(0).cuda()\n",
    "        attr.append(ig.attribute(observation.unsqueeze(0).cuda(), cur_baseline, target=0))\n",
    "        \n",
    "    print(\"--- '%.2f' seconds computation time ---\" % (time.time() - start_time))\n",
    "    return torch.cat(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_gradients_avg(X, baseline, model, sample_size=10):\n",
    "    start_time = time.time()\n",
    "    attr = torch.zeros(X.shape)\n",
    "    attr = attr.cuda()\n",
    "    ig = IntegratedGradients(model)\n",
    "    for i in range(sample_size):\n",
    "        idx = np.random.randint(baseline.shape[0], size=1)\n",
    "        sample = torch.from_numpy(baseline[idx,:])\n",
    "        sample = sample.cuda()\n",
    "        X = X.cuda()\n",
    "        attr += ig.attribute(X, sample, target=0)\n",
    "        \n",
    "    print(\"--- '%.2f' seconds computation time ---\" % (time.time() - start_time))\n",
    "    return (attr/sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_lift(X, baseline, model):\n",
    "    start_time = time.time()\n",
    "    dl = DeepLift(model)\n",
    "    X = X.cuda()\n",
    "    baseline = baseline.cuda()\n",
    "    attr = dl.attribute(X, baseline, target=0)\n",
    "    print(\"--- '%.2f' seconds computation time ---\" % (time.time() - start_time))\n",
    "    return attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_lift_mdb(X, model, X_min, X_max, columns):\n",
    "    start_time = time.time()\n",
    "    dl = DeepLift(model)\n",
    "    attr = []\n",
    "    but = BaselineUtilTensor()\n",
    "    for observation in X:\n",
    "        cur_baseline = but.create_max_dist_baseline(observation, X_min, X_max, columns).unsqueeze(0).cuda()\n",
    "        attr.append(dl.attribute(observation.unsqueeze(0).cuda(), cur_baseline, target=0))\n",
    "    print(\"--- '%.2f' seconds computation time ---\" % (time.time() - start_time))\n",
    "    return torch.cat(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_lift_avg(X, baseline, model, sample_size=10):\n",
    "    start_time = time.time()\n",
    "    attr = torch.zeros(X.shape)\n",
    "    attr = attr.cuda()\n",
    "    dl = DeepLift(model)\n",
    "    for i in range(sample_size):\n",
    "        idx = np.random.randint(baseline.shape[0], size=1)\n",
    "        sample = torch.from_numpy(baseline[idx,:])\n",
    "        sample = sample.cuda()\n",
    "        X = X.cuda()\n",
    "        attr += dl.attribute(X, sample, target=0)\n",
    "        \n",
    "    print(\"--- '%.2f' seconds computation time ---\" % (time.time() - start_time))\n",
    "    return (attr/sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_shap(X, baseline, model):\n",
    "    start_time = time.time()\n",
    "    X = X.cuda()\n",
    "    baseline = baseline.cuda()\n",
    "    explainer = shap.DeepExplainer(model, baseline)\n",
    "    shap_values = explainer.shap_values(X, ranked_outputs=1)\n",
    "    print(\"--- '%.2f' seconds computation time ---\" % (time.time() - start_time))\n",
    "    return shap_values if len(shap_values) == X.shape[0] else shap_values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_shap_mdb(X, model, X_min, X_max, columns):\n",
    "    start_time = time.time()\n",
    "    attr_list = []\n",
    "    but = BaselineUtilTensor()\n",
    "    for observation in X:\n",
    "        cur_baseline = but.create_max_dist_baseline(observation, X_min, X_max, columns).unsqueeze(0).cuda()\n",
    "        explainer = shap.DeepExplainer(model, cur_baseline)\n",
    "        shap_values = explainer.shap_values(observation.unsqueeze(0).cuda(), ranked_outputs=1)\n",
    "        if len(shap_values) == 1:\n",
    "            attr_list.append(shap_values)\n",
    "        else:\n",
    "            attr_list.append(shap_values[0][0])\n",
    "        \n",
    "    print(\"--- '%.2f' seconds computation time ---\" % (time.time() - start_time))\n",
    "    return np.concatenate(attr_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_shap_avg(X, baseline, model, sample_size=10):\n",
    "    start_time = time.time()\n",
    "    attr = np.zeros(X.shape)\n",
    "    X = X.cuda()\n",
    "    for i in range(sample_size):\n",
    "        idx = np.random.randint(baseline.shape[0], size=1)\n",
    "        sample = torch.from_numpy(baseline[idx,:])\n",
    "        sample = sample.cuda()     \n",
    "        explainer = shap.DeepExplainer(model, sample)\n",
    "        shap_values = explainer.shap_values(X, ranked_outputs=1)\n",
    "        if len(shap_values) == X.shape[0]:\n",
    "            attr += shap_values\n",
    "        else:\n",
    "            attr += shap_values[0][0]\n",
    "        \n",
    "    print(\"--- '%.2f' seconds computation time ---\" % (time.time() - start_time))\n",
    "    return (attr/sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ig_attr(X_test_c_tens, black_baseline, blurred_baseline, uniform_baseline, gaussian_baseline, \n",
    "                train_baseline, model, X_min, X_max, columns):\n",
    "    print(\"Start integrated gradients for correct predictions with black_baseline\")\n",
    "    bb_attr = integrated_gradients(X_test_c_tens, black_baseline, model)\n",
    "    print(\"Start integrated gradients for correct predictions with max_dist_baseline\")\n",
    "    mdb_attr = integrated_gradients_mdb(X_test_c_tens, model, X_min, X_max, columns)\n",
    "    print(\"Start integrated gradients for correct predictions with blurred_baseline\")\n",
    "    blb_attr = integrated_gradients_avg(X_test_c_tens, blurred_baseline, model)\n",
    "    print(\"Start integrated gradients for correct predictions with uniform_baseline\")\n",
    "    ub_attr = integrated_gradients_avg(X_test_c_tens, uniform_baseline, model)\n",
    "    print(\"Start integrated gradients for correct predictions with gaussian_baseline\")\n",
    "    gb_attr = integrated_gradients_avg(X_test_c_tens, gaussian_baseline, model)\n",
    "    print(\"Start integrated gradients for correct predictions with train_baseline\")\n",
    "    tb_attr = integrated_gradients_avg(X_test_c_tens, train_baseline, model)\n",
    "    return bb_attr, mdb_attr, blb_attr, ub_attr, gb_attr, tb_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dl_attr(X_test_c_tens, black_baseline, blurred_baseline, uniform_baseline, gaussian_baseline, \n",
    "                train_baseline, model, X_min, X_max, columns):\n",
    "    print(\"Start DeepLIFT for correct predictions with black_baseline\")\n",
    "    bb_attr = deep_lift(X_test_c_tens, black_baseline, model)\n",
    "    print(\"Start DeepLIFT for correct predictions with max_dist_baseline\")\n",
    "    mdb_attr = deep_lift_mdb(X_test_c_tens, model, X_min, X_max, columns)\n",
    "    print(\"Start DeepLIFT for correct predictions with blurred_baseline\")\n",
    "    blb_attr = deep_lift_avg(X_test_c_tens, blurred_baseline, model)\n",
    "    print(\"Start DeepLIFT for correct predictions with uniform_baseline\")\n",
    "    ub_attr = deep_lift_avg(X_test_c_tens, uniform_baseline, model)\n",
    "    print(\"Start DeepLIFT for correct predictions with gaussian_baseline\")\n",
    "    gb_attr = deep_lift_avg(X_test_c_tens, gaussian_baseline, model)\n",
    "    print(\"Start DeepLIFT for correct predictions with train_baseline\")\n",
    "    tb_attr = deep_lift_avg(X_test_c_tens, train_baseline, model)\n",
    "    return bb_attr, mdb_attr, blb_attr, ub_attr, gb_attr, tb_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ds_attr(X_test_c_tens, black_baseline, blurred_baseline, uniform_baseline, gaussian_baseline, \n",
    "                train_baseline, model, X_min, X_max, columns):\n",
    "    print(\"Start DeepSHAP for correct predictions with black_baseline\")\n",
    "    bb_attr = deep_shap(X_test_c_tens, black_baseline, model)\n",
    "    print(\"Start DeepSHAP for correct predictions with max_dist_baseline\")\n",
    "    mdb_attr = deep_shap_mdb(X_test_c_tens, model, X_min, X_max, columns)\n",
    "    print(\"Start DeepSHAP for correct predictions with blurred_baseline\")\n",
    "    blb_attr = deep_shap_avg(X_test_c_tens, blurred_baseline, model)\n",
    "    print(\"Start DeepSHAP for correct predictions with uniform_baseline\")\n",
    "    ub_attr = deep_shap_avg(X_test_c_tens, uniform_baseline, model)\n",
    "    print(\"Start DeepSHAP for correct predictions with gaussian_baseline\")\n",
    "    gb_attr = deep_shap_avg(X_test_c_tens, gaussian_baseline, model)\n",
    "    print(\"Start DeepSHAP for correct predictions with train_baseline\")\n",
    "    tb_attr = deep_shap_avg(X_test_c_tens, train_baseline, model)\n",
    "    return bb_attr, mdb_attr, blb_attr, ub_attr, gb_attr, tb_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attr_scores(method, X_test_c_tens, black_baseline, blurred_baseline, uniform_baseline, gaussian_baseline, \n",
    "                train_baseline, model, X_min, X_max, columns):\n",
    "    options = {\n",
    "        'IG': calc_ig_attr,\n",
    "        'DeepLIFT': calc_dl_attr,\n",
    "        'DeepSHAP': calc_ds_attr\n",
    "    }\n",
    "    return options[method](X_test_c_tens, black_baseline, blurred_baseline, uniform_baseline, gaussian_baseline, \n",
    "                train_baseline, model, X_min, X_max, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_predictions(model, X_test, X_test_tens, Y_test, averaging):\n",
    "    np.random.seed(rs)\n",
    "    random.seed(rs)\n",
    "    \n",
    "    Y_test_reset = Y_test.reset_index(drop=True)\n",
    "    corr_idx = []\n",
    "\n",
    "    predictions = model(X_test_tens.cuda())\n",
    "    y_pred_label = torch.round(torch.sigmoid(predictions)) if averaging == 'binary' else torch.max(predictions.data, 1)[1]\n",
    "\n",
    "    xs = np.asarray(X_test, dtype=np.float32)\n",
    "    ys = np.asarray(Y_test, dtype=np.int) if averaging == 'binary' else np.asarray(Y_test, dtype=np.int) - 1\n",
    "\n",
    "    for x, prediction, y in zip(enumerate(xs), y_pred_label, ys):\n",
    "        if prediction == y:\n",
    "            corr_idx.append(x[0])\n",
    "\n",
    "    X_test_c = X_test[X_test.index.isin(corr_idx)]\n",
    "    print(f\"X_test of correct predictions shape: {X_test_c.shape}\")\n",
    "    Y_test_c = Y_test_reset[Y_test_reset.index.isin(corr_idx)] if averaging == 'binary' else Y_test_reset[Y_test_reset.index.isin(corr_idx)] - 1\n",
    "    print(f\"Y_test of correct predictions shape: {Y_test_c.shape}\")\n",
    "    unique, counts = np.unique(Y_test_c, return_counts=True)\n",
    "    print(f\"Label in Y_test of correct predictions ratio: \\n {np.asarray((unique, counts)).T}\")\n",
    "\n",
    "    return X_test_c, Y_test_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_ablation_row_wise(model, X, y, baseline_attr, k, imputation, averaging):\n",
    "    if k >= 1.0:\n",
    "        amount = len(X.columns)\n",
    "    else:\n",
    "        amount = int(len(X.columns) * k)\n",
    "    columns = X.columns\n",
    "    attr_abs = np.abs(baseline_attr)\n",
    "    if imputation.ndim == 1:\n",
    "        for row_x, row_shap in zip(X.iterrows(), attr_abs):\n",
    "            row_x = row_x[1]\n",
    "            sorted_attr = np.argsort(-row_attr, axis=0)\n",
    "            for i in range(0, amount):\n",
    "                row_x[columns[sorted_attr[i]]] = imputation[columns[sorted_attr[i]]]\n",
    "    elif imputation.ndim == 2:\n",
    "        for row_x, row_attr, row_imp in zip(X.iterrows(), attr_abs, imputation.iterrows()):\n",
    "            row_x, row_imp = row_x[1], row_imp[1]\n",
    "            sorted_attr = np.argsort(-row_attr, axis=0)\n",
    "            for i in range(0, amount):\n",
    "                row_x[columns[sorted_attr[i]]] = row_imp[columns[sorted_attr[i]]]\n",
    "                \n",
    "    else:\n",
    "        raise Exception('ndim of imputation too high, allowed 1 or 2')\n",
    "    \n",
    "    y_pred = model(torch.from_numpy(X.to_numpy(dtype=np.float32)).cuda())\n",
    "    y_hat = torch.round(torch.sigmoid(y_pred)) if averaging == 'binary' else torch.max(y_pred.data, 1)[1]\n",
    "    return f1_score(y, y_hat.cpu().data.numpy(), average=averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_k_ablation_row_wise(model, X, y, k, imputation, averaging):\n",
    "    if k >= 1.0:\n",
    "        amount = len(X.columns)\n",
    "    else:\n",
    "        amount = int(len(X.columns) * k)       \n",
    "    columns = X.columns\n",
    "    rnd_columns = random.sample(range(0, len(X.columns)), amount)\n",
    "    if imputation.ndim == 1:\n",
    "        for i in rnd_columns:\n",
    "            X[columns[i]] = imputation[columns[i]]\n",
    "    elif imputation.ndim == 2:\n",
    "        for row_x, row_imp in zip(X.iterrows(), imputation.iterrows()):\n",
    "            row_x, row_imp = row_x[1], row_imp[1]\n",
    "            for i in rnd_columns:\n",
    "                row_x[columns[i]] = row_imp[columns[i]]\n",
    "    else:\n",
    "        raise Exception('ndim of imputation too high, allowed 1 or 2')\n",
    "    y_pred = model(torch.from_numpy(X.to_numpy(dtype=np.float32)).cuda())\n",
    "    y_hat = torch.round(torch.sigmoid(y_pred)) if averaging == 'binary' else torch.max(y_pred.data, 1)[1]\n",
    "    return f1_score(y, y_hat.cpu().data.numpy(), average=averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_dict = defaultdict(list)\n",
    "datasets_names = ['har', 'compas', 'communities', 'fraud_detection', 'spambase']\n",
    "datasets_paths = ['har', 'compas/', 'crime/', 'fraud_detection/', 'spambase/']\n",
    "method = 'DeepLIFT'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for dataset_name, dataset_path in zip(datasets_names, datasets_paths):\n",
    "    np.random.seed(rs)\n",
    "    torch.manual_seed(rs)\n",
    "    random.seed(rs)\n",
    "   \n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    plt.gca().xaxis.set_major_formatter(PercentFormatter(1))\n",
    "    plt.grid()\n",
    "        \n",
    "    print(f\"Start iteration for dataset '{dataset_name}'\")\n",
    "    data = f'datasets/{dataset_path}' if dataset_name == 'har' else pd.read_csv('datasets/' + dataset_path + dataset_name + '.' + 'csv')\n",
    "    X_train, X_test, Y_train, Y_test = dataloader.prepare_dataset(dataset_name, data)\n",
    "    X_train_tens = torch.tensor(X_train.to_numpy()).float()\n",
    "    X_test_tens = torch.tensor(X_test.to_numpy()).float()\n",
    "    \n",
    "    unique, counts = np.unique(Y_train, return_counts=True)\n",
    "    \n",
    "    if (len(counts) > 2):\n",
    "        averaging = 'macro'\n",
    "        y_train_tens = torch.tensor(Y_train.to_numpy()).long() - 1\n",
    "        output_dim = len(counts)\n",
    "    else:\n",
    "        averaging = 'binary'\n",
    "        y_train_tens = torch.tensor(Y_train.to_numpy()).view(-1, 1).float()\n",
    "        output_dim = 1\n",
    "        \n",
    "    dataset_tens = torch.utils.data.TensorDataset(X_train_tens, y_train_tens)\n",
    "    train_iter = torch.utils.data.DataLoader(dataset_tens, batch_size, shuffle=False)\n",
    "            \n",
    "    print(\"Try to load the model..\")\n",
    "    model = nn_model.get_model(device, train_iter, X_train.shape[1], output_dim, averaging, learning_rate, num_epochs)\n",
    "      \n",
    "    print(\"Loading all baselines..\")\n",
    "    bun = BaselineUtilTensor()\n",
    "    \n",
    "    black_baseline = bun.create_black_baseline(X_train)\n",
    "    blurred_baseline = bun.create_blurred_baseline(X_train, 10)\n",
    "    uniform_baseline = bun.create_uniform_baseline(X_train)\n",
    "    gaussian_baseline = bun.create_gaussian_baseline(X_train, 0.5)\n",
    "    train_baseline = bun.create_train_baseline(X_train)\n",
    "        \n",
    "    print(\"Get correct predictions..\")\n",
    "    X_test_c, Y_test_c = get_correct_predictions(model, X_test, X_test_tens, Y_test, averaging)\n",
    "    X_test_c_tens = torch.from_numpy(X_test_c.to_numpy(dtype=np.float32)).cuda()\n",
    "    \n",
    "    #ks for the top k ablation test\n",
    "    ks = np.arange(start=0.0, stop=1.1, step=0.1)\n",
    "    \n",
    "    np.random.seed(rs)\n",
    "    torch.manual_seed(rs)\n",
    "    print(\"Get all attr. scores for the different baselines..\")\n",
    "    bb_attr, mdb_attr, blb_attr, ub_attr, gb_attr, tb_attr = get_attr_scores(method, X_test_c_tens, black_baseline,\n",
    "                                                                             blurred_baseline, uniform_baseline,\n",
    "                                                                             gaussian_baseline, train_baseline, model,\n",
    "                                                                            X_test_c.min(), X_test_c.max(), X_test_c.columns)\n",
    "    attribute_scores = {\n",
    "          \"Constant Baseline\": bb_attr,\n",
    "          \"Maximum Distance Baseline\": mdb_attr,\n",
    "          \"Blurred Baseline\": blb_attr,\n",
    "          \"Uniform Baseline\": ub_attr,\n",
    "          \"Gaussian Baseline\": gb_attr,\n",
    "          \"Expectation Baseline\": tb_attr\n",
    "    }\n",
    "        \n",
    "    print(\"Start Top-K Ablation..\")\n",
    "    imputation = imp.load_imputation(imputation_typ, X_test_c)\n",
    "    test_errors = []\n",
    "    for name, score in attribute_scores.items():\n",
    "        for k in ks:\n",
    "            key = name + str(round(k,1))\n",
    "            if method == 'DeepSHAP':\n",
    "                error = top_k_ablation_row_wise(model, X_test_c.copy(deep=True), Y_test_c, score, k, imputation, averaging)\n",
    "                test_errors.append(error)\n",
    "                #For the avg plots\n",
    "                errors_dict[key].append(error)\n",
    "            else:\n",
    "                error = top_k_ablation_row_wise(model, X_test_c.copy(deep=True), Y_test_c, score.cpu().data.numpy(), k, imputation, averaging)\n",
    "                test_errors.append(error)\n",
    "                errors_dict[key].append(error)\n",
    "                \n",
    "        plt.plot(ks, test_errors, label=name, linewidth=2.5, alpha=0.7)\n",
    "        test_errors.clear()\n",
    "    \n",
    "    for k in ks:\n",
    "        key = 'Random Ablation' + str(round(k,1))\n",
    "        error = random_k_ablation_row_wise(X_test_c.copy(deep=True), Y_test_c, k, imputation, averaging)\n",
    "        test_errors.append(error)\n",
    "        errors_dict[key].append(error)\n",
    "                \n",
    "    plt.plot(ks, test_errors, label='Random Ablation', linewidth=2.5, alpha=0.7)\n",
    "    test_errors.clear()\n",
    "        \n",
    "    plt.xlabel('Ablation percentage of the most important features determined by the baseline',fontsize=14)\n",
    "    plt.ylabel('F1 score as fraction of original F1 score', fontsize=14)\n",
    "    plt.legend(prop={'size': 13})\n",
    "    fig.savefig(f\"plots/top_k_test_{method}_{dataset_name}_nn.pdf\", dpi=fig.dpi,\n",
    "                format='pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create overall AVG plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_names = [\"Constant Baseline\",\n",
    "\"Maximum Distance Baseline\",\n",
    "\"Blurred Baseline\",\n",
    "\"Uniform Baseline\",\n",
    "\"Gaussian Baseline\",\n",
    "\"Expectation Baseline\",\n",
    "\"Random Ablation\"]\n",
    "\n",
    "test_errors = defaultdict(list)\n",
    "test_std = defaultdict(list)\n",
    "\n",
    "for b in baseline_names:\n",
    "    for k in ks:\n",
    "        key = b + str(round(k,1))\n",
    "        test_errors[b].append(np.mean(errors_dict[key]))\n",
    "        test_std[b].append(np.std(errors_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.gca().xaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.grid()\n",
    "\n",
    "for b in baseline_names:\n",
    "    y = np.asarray(test_errors[b])\n",
    "    std = np.asarray(test_std[b])\n",
    "    plt.plot(ks, y, label=b, linewidth=2.5, alpha=0.7)\n",
    "    plt.fill_between(ks, y-(std/2), y+(std/2), alpha=0.2)\n",
    "\n",
    "plt.xlabel('Ablation percentage of the most important features determined by the baseline',fontsize=14)\n",
    "plt.ylabel('F1 score as fraction of original F1 score', fontsize=14)\n",
    "plt.legend(prop={'size': 13})\n",
    "fig.savefig(f\"plots/top_k_test_AVG_{method}_w_shader.pdf\", dpi=fig.dpi,\n",
    "            format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.gca().xaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.grid()\n",
    "\n",
    "for b in baseline_names:\n",
    "    y = np.asarray(test_errors[b])\n",
    "    std = np.asarray(test_std[b])\n",
    "    plt.plot(ks, y, label=b, linewidth=2.5, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Ablation percentage of the most important features determined by the baseline',fontsize=14)\n",
    "plt.ylabel('F1 score as fraction of original F1 score', fontsize=14)\n",
    "plt.legend(prop={'size': 13})\n",
    "fig.savefig(f\"plots/top_k_test_AVG_{method}.pdf\", dpi=fig.dpi,\n",
    "            format='pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
